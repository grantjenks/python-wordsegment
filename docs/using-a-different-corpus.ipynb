{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Different Corpus\n",
    "\n",
    "WordSegment makes it easy to use a different corpus for word segmentation.\n",
    "\n",
    "If you simply want to \"teach\" the algorithm a single phrase it doesn't know then read [this StackOverflow answer](http://stackoverflow.com/questions/20695825/english-word-segmentation-in-nlp).\n",
    "\n",
    "Now, let's get a new corpus. For this example, we'll use the text from Jane Austen's *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717573\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://www.gutenberg.org/ebooks/1342.txt.utf-8')\n",
    "\n",
    "text = response.text\n",
    "\n",
    "print len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We've got a new corpus for `wordsegment`. Now let's look at what parts of the API we need to change. There's one function and two dictionaries: `wordsegment.clean`, `wordsegment.bigram_counts` and `wordsegment.unigram_counts`. We'll work on these in reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wordsegment.unigram_counts), type(wordsegment.bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('biennials', 37548.0), ('verplank', 48349.0), ('tsukino', 19771.0)]\n",
      "[('personal effects', 151369.0), ('basic training', 294085.0), ('it absolutely', 130505.0)]\n"
     ]
    }
   ],
   "source": [
    "print wordsegment.unigram_counts.items()[:3]\n",
    "print wordsegment.bigram_counts.items()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so `wordsegment.unigram_counts` is just a dictionary mapping unigrams to their counts. Let's write a method to tokenize our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wait', 'what', 'did', 'you', 'say']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile('[a-zA-Z]+')\n",
    "    return (match.group(0) for match in pattern.finditer(text))\n",
    "\n",
    "print list(tokenize(\"Wait, what did you say?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "wordsegment.unigram_counts = Counter(tokenize(text))\n",
    "\n",
    "def pairs(iterable):\n",
    "    iterator = iter(iterable)\n",
    "    values = [next(iterator)]\n",
    "    for value in iterator:\n",
    "        values.append(value)\n",
    "        yield ' '.join(values)\n",
    "        del values[0]\n",
    "\n",
    "wordsegment.bigram_counts = Counter(pairs(tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.\n",
    "\n",
    "Now, by default, `wordsegment.segment` lowercases all input and removes punctuation. In our corpus we have capitals so we'll also have to change the `clean` function. Our heaviest hammer is to simply replace it with the identity function. This will do no sanitation of the input to `segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def identity(value):\n",
    "    return value\n",
    "\n",
    "wordsegment.clean = identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['want', 'of', 'a', 'wife']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsegment.segment('wantofawife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find this behaves poorly then you may need to change the `wordsegment.TOTAL` variable to reflect the total of all unigrams. In our case that's simply: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsegment.TOTAL = float(sum(wordsegment.unigram_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordSegment doesn't require any fancy machine learning training algorithms. Simply update the unigram and bigram count dictionaries and you're ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
