{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Load Dict Fast from File\n",
    "\n",
    "Python `wordsegment` uses two text files to store unigram and bigram count data. The files currently store records separated by newline characters with fields separated by tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the\\t23135851162\\n'\n"
     ]
    }
   ],
   "source": [
    "with open('../wordsegment_data/unigrams.txt', 'r') as reader:\n",
    "    print repr(reader.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `wordsegment` module is imported these files are read from disk and used to construct a Python `dict` mapping `word` to `count` pairs.\n",
    "\n",
    "That function works like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 286 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = (line.split('\\t') for line in reader)\n",
    "    dict((word, float(number)) for word, number in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're talking about performance, here's some details about my platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n",
      "\n",
      "2.7.10 (default, May 25 2015, 13:06:17) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.56)]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print subprocess.check_output([\n",
    "    '/usr/sbin/sysctl', '-n', 'machdep.cpu.brand_string'\n",
    "])\n",
    "\n",
    "import sys\n",
    "print sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the files in about a second is plenty fast for me but I wondered if there was a faster way. Here's a few things I tried.\n",
    "\n",
    "Simply reading all the lines from the file takes 27ms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 26.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = [line for line in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to accomplish the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 20.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = reader.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 30% faster but it's a small part of 286ms. What takes the majority of the time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 115 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = (line.split('\\t') for line in reader)\n",
    "    for word, number in lines:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So splitting each line takes nearly 90ms. That's a bit surprising to me. What else takes so long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 167 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = (line.split('\\t') for line in reader)\n",
    "    for word, number in lines:\n",
    "        float(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 51ms to convert strings to floats. Maybe later we can optimize that. Finally, the last chunk must be constructing the `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 254 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = (line.split('\\t') for line in reader)\n",
    "    result = dict()\n",
    "    for word, number in lines:\n",
    "        result[word] = float(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `__setitem__` repeatedly we avoid the construction of the tuple used to construct the dict using its constructor. Let's experiment with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 303 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = (line.split('\\t') for line in reader)\n",
    "    dict((word, float(number)) for word, number in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't Python 2.6 compatible but what about a `dict` comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 275 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    lines = (line.split('\\t') for line in reader)\n",
    "    {word: float(number) for word, number in lines}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit disappointing that the constructor is slower than calling `__setitem__` repeatedly. But maybe that just reflects how much optimization has gone into making `__setitem__` really fast.\n",
    "\n",
    "Here's a breakdown of how long various steps are taking:\n",
    "\n",
    "| Operation | Time |\n",
    "| --------- | ---- |\n",
    "| Read file and parse lines | 26ms |\n",
    "| Split lines by tab character | 90ms |\n",
    "| Convert strings to floats | 50ms |\n",
    "| Creating `dict(...)` | 135ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, constructing the `dict` is hard to optimize. So let's look at the other steps. If we stored the counts on disk in binary format then we could avoid parsing them. If we did so, we might likewise store the words in a separate file. Let's convert our unigrams file into two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../wordsegment_data/unigrams.txt') as reader:\n",
    "    pairs = [line.split('\\t') for line in reader]\n",
    "    words = [pair[0] for pair in pairs]\n",
    "    counts = [float(pair[1]) for pair in pairs]\n",
    "    \n",
    "    with open('words.txt', 'wb') as writer:\n",
    "        writer.write('\\n'.join(words))\n",
    "        \n",
    "    from array import array\n",
    "    values = array('d')\n",
    "    values.fromlist(counts)\n",
    "    with open('counts.bin', 'wb') as writer:\n",
    "        values.tofile(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two files: `words.txt` and `counts.bin`. The first stores words separated by newline characters in ascii. The latter stores double-precision floating-point numbers in binary. Together we can use these to construct our `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip as zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 106 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('words.txt', 'rb') as lines, open('counts.bin', 'rb') as counts:\n",
    "    words = lines.read().split('\\n')\n",
    "    values = array('d')\n",
    "    values.fromfile(counts, 333333)\n",
    "    dict(zip(words, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. We started at a time of 286ms and worked down to 106ms. That's 62% faster. The key to the speedup is separating the `dict` keys and values and using fast methods for parsing each. Reading words from a file now uses `str.split` which is actually faster than Python's built-in buffered-file readline mechanism. The `array` module parses counts directly from a binary-formatted file. Finally, the `dict` constructor is used with arguments izipped together. I tried using the `__setitem__` trick here but results were within error of one another and I prefer this style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, I'm not that impressed. 62% is faster but I expected to improve things by 10x not 2x. Even with this speedup, you'll notice a delay on module import. And now the format of the files is funky. They don't play nice with grep, etc. I'm going to leave things as-is for now.\n",
    "\n",
    "I'd be happy to hear what others have tried. Note in this case that I don't care how long it takes to write the files. That would be another interesting thing to benchmark.\n",
    "\n",
    "I also tried formatting the `dict` in a Python module which would be parsed on import. This was actually a little slower than the initial code. My guess is the Python interpreter is doing roughly the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
